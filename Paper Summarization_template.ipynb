{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: left; font-size: 30px;\">Paper Summarization Bot ü§ñ</div>\n",
    "<div style=\"text-align: left; font-size: 20px; color: gray;\">Supported by <strong>LangChain</strong> and <strong>IBM Watsonx</strong></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "1. [Load PDF (paper as you want)](#load-pdf)\n",
    "2. [Connect to IBM Watsonx.ai and Initialize LLM Model](#connect-ibm-watsonx)\n",
    "3. [Make Semantic Chunking Using Watsonx Embedding Model](#semantic-chunking)\n",
    "4. [Apply Semantic Chunk](#apply-semantic-chunk)\n",
    "5. [Map Reduce Using LangChain](#map-reduce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load-pdf\"></a>\n",
    "## Load PDF (paper as you want)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "c:\\AICA-mentoring-program\\venv\\Lib\\site-packages\\ibm_watson_machine_learning\\foundation_models\\extensions\\langchain\\llm.py:60: WatsonxLLMDeprecationWarning: ibm_watson_machine_learning.foundation_models.extensions.langchain.WatsonxLLM is deprecated and will not be supported in the future. Please import from langchain-ibm instead.\n",
      "To install langchain-ibm run `pip install -U langchain-ibm`.\n",
      "  _raise_watsonxllm_deprecation_warning()\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from pydantic import field_validator\n",
    "from langchain import PromptTemplate # Langchain Prompt Template\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain # Langchain Chains\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyPDFLoaderÎ•º Ïù¥Ïö©ÌïòÏó¨ Î¨∏ÏÑúÎ•º Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "def load_pdf_to_doc(file_name):\n",
    "    loader = PyPDFLoader(file_name)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "pdf_file = \"yolo.pdf\"\n",
    "documents = load_pdf_to_doc(f\"./{pdf_file}\")\n",
    "\n",
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"connect-ibm-watsonx\"></a>\n",
    "## Connect to IBM Watsonx.ai and initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Load the environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# regionÏóê Îî∞Îùº Ï£ºÏÜåÍ∞Ä Îã§Î•º Ïàò ÏûàÏäµÎãàÎã§. Ï£ºÏÜåÎ•º ÌôïÏù∏Ìï¥ Ï£ºÏÑ∏Ïöî.\n",
    "ibm_cloud_url = \"https://us-south.ml.cloud.ibm.com\" \n",
    "project_id=os.environ['PROJECT_ID']\n",
    "api_key = os.environ['API_KEY']\n",
    "\n",
    "\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    raise Exception(\"One or more environment variables are missing!\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done initializing LLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\AICA-mentoring-program\\venv\\Lib\\site-packages\\ibm_watson_machine_learning\\foundation_models\\utils\\utils.py:273: LifecycleWarning: Model 'meta-llama/llama-3-70b-instruct' is in deprecated state from 2024-12-02 until 2025-02-03. IDs of alternative models: None. Further details: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-model-lifecycle.html?context=wx&audience=wdp\n",
      "  warnings.warn(default_warning_template.format(\n"
     ]
    }
   ],
   "source": [
    "# watsonx model Ï¥àÍ∏∞Ìôî\n",
    "params = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",\n",
    "    GenParams.REPETITION_PENALTY: 1.1,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.MAX_NEW_TOKENS: 1024\n",
    "}\n",
    "\n",
    "llm_model = Model(\n",
    "    model_id=\"meta-llama/llama-3-70b-instruct\",\n",
    "    params=params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id\n",
    ")\n",
    "\n",
    "print(\"Done initializing LLM.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Check if the llm works\n",
    "# llm_model.generate(\"User: hello. nice to meet you\\n Agent:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"semantic-chunking\"></a>\n",
    "## Make Semantic Chunking Using Watsonx Embedding Model\n",
    "- Semantic chunking options\n",
    "    - `percentile` (default) ‚Äî In this method, all differences between sentences are calculated, and then any difference greater than the X percentile is split.\n",
    "\n",
    "    - `standard_deviation` ‚Äî In this method, any difference greater than X standard deviations is split.\n",
    "\n",
    "    - `interquartile` ‚Äî In this method, the interquartile distance is used to split chunks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¥àÍ∏∞Ìôî\n",
    "from ibm_watsonx_ai import Credentials\n",
    "from ibm_watsonx_ai.foundation_models import Embeddings\n",
    "from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames as EmbedParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import EmbeddingTypes\n",
    "\n",
    "embed_params = {\n",
    "     EmbedParams.TRUNCATE_INPUT_TOKENS: 3,\n",
    "     EmbedParams.RETURN_OPTIONS: {\n",
    "     'input_text': True\n",
    "     }\n",
    " }\n",
    "\n",
    "embedding = Embeddings(\n",
    "    model_id=\"intfloat/multilingual-e5-large\",\n",
    "    params=embed_params,\n",
    "    credentials=Credentials(\n",
    "        api_key = api_key,\n",
    "        url = \"https://us-south.ml.cloud.ibm.com\"),\n",
    "    project_id=project_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"apply-semantic-chunk\"></a>\n",
    "## Apply Semantic Chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    }
   ],
   "source": [
    "#Semantic Chunk ÎßåÎì§Í∏∞ \n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "semantic_chunker = SemanticChunker(embedding, breakpoint_threshold_type=\"percentile\")\n",
    "#\n",
    "semantic_chunks = semantic_chunker.create_documents([d.page_content for d in documents])\n",
    "\n",
    "# # Enable the following if you want to see the content of the semantic chunks\n",
    "#for semantic_chunk in semantic_chunks:\n",
    "#    print(semantic_chunk.page_content)\n",
    "#    print(len(semantic_chunk.page_content))\n",
    "    \n",
    "print(len(semantic_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"map-reduce\"></a>\n",
    "## Map Reduce Using LangChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïª§Ïä§ÌÖÄ LLM ÎßåÎì§Í∏∞\n",
    "from langchain.llms.base import LLM\n",
    "from pydantic import Field\n",
    "\n",
    "class CustomWatsonLLM(LLM):\n",
    "    model: object = Field(...)  # Define model as a field\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__(model=model)\n",
    "\n",
    "    def _call(self, prompt, stop=None):\n",
    "        # Use the generate method from your model\n",
    "        response = self.model.generate(prompt)\n",
    "        return response['results'][0]['generated_text']\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom_watson_llm\"\n",
    "\n",
    "# Instantiate your custom LLM\n",
    "custom_llm = CustomWatsonLLM(llm_model)\n",
    "\n",
    "# # Now you can use this with LLMChain\n",
    "# reduce_chain = LLMChain(llm=custom_llm, prompt=reduce_prompt)\n",
    "# reduce_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init map chain...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18472\\1145923435.py:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  map_chain = LLMChain(llm=custom_llm, prompt=map_prompt)\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18472\\1145923435.py:32: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_documents_chain = StuffDocumentsChain(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18472\\1145923435.py:37: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n",
      "  reduce_documents_chain = ReduceDocumentsChain(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18472\\1145923435.py:44: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n",
      "  map_reduce_chain = MapReduceDocumentsChain(\n",
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_18472\\1145923435.py:58: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = map_chain({\"docs\": chunk.page_content})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init reduce chain...\n",
      "Stuff documents using reduce chain...\n",
      "Running map phase...\n",
      "\u001b[32m\n",
      "Intermediate map result 1:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors introduce YOLO (You Only Look Once), a novel approach to object detection that differs from prior work which uses classifiers for detection.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 2:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors propose a new object detection algorithm called YOLO (You Only Look Once), which frames object detection as a regression problem and predicts bounding boxes and class probabilities directly from full images in one evaluation. This allows for fast and accurate object detection, making it suitable for applications such as self-driving cars and assistive devices.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 3:\u001b[0m\n",
      " \n",
      "The main point of this section is that traditional object detection methods involve multiple complex steps, whereas the authors propose a simpler approach called YOLO (You Only Look Once), which reframes object detection as a single regression problem that can predict bounding boxes and class probabilities directly from image pixels.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 4:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO (You Only Look Once) is an efficient and effective object detection method that outperforms traditional approaches in terms of speed and accuracy. It achieves high-speed processing of up to 150 frames per second while maintaining high mean average precision.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 5:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO (You Only Look Once) is a novel object detection algorithm that has several advantages over traditional methods, including its ability to make accurate predictions, learn generalizable representations of objects, and reason globally about the full image.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 6:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors propose a novel object detection algorithm called YOLO (You Only Look Once), which detects objects by dividing the image into a grid and predicting bounding boxes, confidence scores, and class probabilities for each grid cell. This approach allows for fast and accurate object detection in images.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 7:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors describe their network design for object detection, which is based on a convolutional neural network (CNN) inspired by GoogLeNet, and they also introduce a faster version called Fast YOLO.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 8:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors compare YOLO (You Only Look Once) and Fast YOLO models under identical conditions, except for the network size.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 9:\u001b[0m\n",
      " The main point of this section is that it describes the architecture of a convolutional neural network (CNN) model, which consists of multiple layers including convolutional layers and max-pooling layers, to process input data.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 10:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors propose an object detection system called YOLO (You Only Look Once), which uses a single neural network to predict bounding boxes and class probabilities directly from full images. The system consists of 24 convolutional layers followed by 2 fully connected layers, and is trained using a combination of classification and detection losses.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 11:\u001b[0m\n",
      " The main point of this section is that during training, the model should assign only one bounding box predictor to each object. This ensures that multiple predictors are not simultaneously responsible for detecting the same object.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 12:\u001b[0m\n",
      " \n",
      "The main point of this section is that assigning one predictor to be responsible for predicting an object based on its highest IOU with the ground truth leads to specialization among predictors and improves overall recall.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 13:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors propose a novel loss function for object detection tasks, which combines classification and localization losses, and describe their training procedure, including hyperparameter settings and data augmentation techniques.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 14:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO's inference process is fast and efficient because it only requires a single network evaluation, unlike other object detection systems. Additionally, the authors discuss the limitations of YOLO, including its struggle with small objects, unusual aspect ratios, and coarse features.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 15:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors are comparing their YOLO (You Only Look Once) object detection system to other top detection frameworks, specifically deformable parts models (DPM) and R-CNN, highlighting the differences and advantages of their unified architecture.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 16:\u001b[0m\n",
      " \n",
      "The main point of this section is that R-CNN and its variants have improved object detection by using region proposals instead of traditional sliding window approaches. This allows for more efficient and effective identification of objects within images.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 17:\u001b[0m\n",
      " The main point of this section is that traditional object detection systems are complex pipelines that require precise tuning of each stage, leading to slow performance. In contrast, YOLO (You Only Look Once) is a fast and accurate object detection system that integrates all stages into a single, jointly optimized model.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 18:\u001b[0m\n",
      " \n",
      "The main point of this section is that Sermanet et al. developed a method for object detection by first training a CNN for localization and then adapting it for detection tasks.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 19:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO (You Only Look Once) is a real-time object detection system that outperforms other existing real-time systems in terms of accuracy and speed. The authors compare YOLO's performance with other systems such as OverFeat, MultiGrasp, and Fast R-CNN, highlighting its advantages in terms of efficiency and effectiveness.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 20:\u001b[0m\n",
      " The main point of this section is that YOLO (You Only Look Once) achieves high accuracy (mAP of 63.4%) while maintaining real-time performance, outperforming other object detection systems such as VGG-16 and Fastest DPM.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 21:\u001b[0m\n",
      " \n",
      "The main point of this section is that Fast YOLO is the fastest object detector on record for PASCAL VOC detection, achieving a high accuracy while maintaining a fast processing speed of 155 frames per second (FPS).\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 22:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO outperforms other object detection systems such as R-CNN and Faster R-CNN in terms of speed and accuracy, achieving 63.4% mAP while running at 45 frames per second. However, YOLO still lags behind these systems in terms of localization accuracy, with a higher proportion of localization errors compared to Fast R-CNN.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 23:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO outperforms Fast R-CNN in terms of localization accuracy, but Fast R-CNN has higher overall accuracy due to fewer background errors. However, when combining the strengths of both models, the resulting hybrid model achieves a significant boost in performance.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 24:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO can be combined with other object detection models like Fast R-CNN to improve their performance, achieving state-of-the-art results on the PASCAL VOC 2012 dataset. This combination leverages the strengths of both models, with YOLO's speed and Fast R-CNN's accuracy.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 25:\u001b[0m\n",
      " The main point of this section is that the authors report the performance of their model (YOLO) on the VOC 2012 test set, achieving a mean average precision (mAP) score of 57.9%.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 26:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors evaluate their YOLO (You Only Look Once) object detection system against other state-of-the-art detectors such as R-CNN and Feature Edit, showing competitive results on various benchmarks, including VOC 2007 and artwork datasets. They also demonstrate YOLO's ability to generalize to new domains and maintain real-time performance in practical applications.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 27:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO (You Only Look Once), a unified model for object detection, outperforms other models such as RCNN, DPM, Poselets, and D&T in terms of accuracy and speed, achieving real-time object detection.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 28:\u001b[0m\n",
      " \n",
      "The main point of this section is that YOLO (You Only Look Once) is an effective object detection system that can generalize well to new domains, making it suitable for applications requiring fast and robust object detection.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 29:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors are citing various research papers related to object detection, pose estimation, and image recognition, highlighting their contributions to the field. These references provide a foundation for the authors' own work, which likely builds upon or contrasts with these existing studies.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 30:\u001b[0m\n",
      " \n",
      "The main point of this section is that it discusses the use of deep learning models for image classification tasks, specifically highlighting the work by Yagnik et al. (2013) which achieved fast and accurate detection of 100,000 object classes using a single machine.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 31:\u001b[0m\n",
      " \n",
      "The main point of this section is that the author presents an overview of the research background and significance of studying the topic.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 32:\u001b[0m\n",
      " The main point of this section is that the authors propose a new approach to image captioning using sequence-to-sequence models and attention mechanisms, which allows for more accurate and diverse captions.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 33:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors are referencing previous research papers related to object detection and image classification tasks, specifically highlighting the use of deep learning techniques such as convolutional neural networks (CNNs) and activation features.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 34:\u001b[0m\n",
      " \n",
      "The main point of this section is that it provides references to various research papers related to object detection using deep neural networks, highlighting their contributions to the field. These papers discuss different approaches to object detection, including scalable object detection, part-based models, and challenges in visual object classes.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 35:\u001b[0m\n",
      " \n",
      "The main point of this section is that it discusses object detection using a multi-region and semantic segmentation-aware CNN model. The authors propose a new approach to improve object detection by incorporating semantic segmentation information into the detection process.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 36:\u001b[0m\n",
      " \n",
      "The main point of this section is that it provides a list of references cited in the paper, which are related to computer vision, object detection, and image segmentation using deep learning techniques such as Fast R-CNN and spatial pyramid pooling.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 37:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors are citing various research papers related to object detection, neural networks, and image processing, highlighting their contributions to the field. These references provide context and support for the authors' own work, which builds upon these existing ideas and techniques.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 38:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors are presenting the performance results of their models on the ImageNet 2012 validation dataset.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 39:\u001b[0m\n",
      " \n",
      "The main point of this section is that it provides references to various papers and resources related to object detection, including models like YOLO, Faster R-CNN, and others, as well as frameworks like Darknet and Caffe. These references are likely used to support the development or comparison of an object detection system.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 40:\u001b[0m\n",
      " The main point of this section is that the authors are citing two papers related to image recognition challenges, specifically the ImageNet Large Scale Visual Recognition Challenge and another unnamed paper by Sadeghi and Forsyth.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 41:\u001b[0m\n",
      " \n",
      "The main point of this section is that it presents an approach to achieve 30Hz object detection using DPM V5.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 42:\u001b[0m\n",
      " \n",
      "The main point of this section is that it discusses the use of deep learning techniques, specifically convolutional neural networks (CNNs), for object detection tasks. The authors reference various papers that have made significant contributions to the field, including the development of the OverFeat model, which integrates recognition, localization, and detection capabilities.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 43:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors are discussing various techniques to improve object detection using convolutional neural networks (CNNs), including modifying the dropout rates in different layers and using different architectures such as GoogLeNet.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 44:\u001b[0m\n",
      " The main point of this section is that the author presents an overview of the research methodology used to investigate the impact of social media on mental health.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 45:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors propose an architecture for image recognition using deep neural networks with convolutional layers, which achieves state-of-the-art performance on several benchmark datasets.\n",
      "\n",
      "\n",
      "\u001b[32m\n",
      "Intermediate map result 46:\u001b[0m\n",
      " \n",
      "The main point of this section is that the authors are discussing various methods and techniques used in object detection, including selective search, edge boxes, and deformable part models, and referencing relevant research papers to support their discussion.\n",
      "\n",
      "\n",
      "Running reduce phase...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2528 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for reduce phase: 116.12 seconds.\n",
      "\n",
      "\u001b[36m+----------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\u001b[36m \n",
      "\n",
      "The final summarization of this paper is that it introduces YOLO (You Only Look Once), a novel approach to object detection that frames object detection as a regression problem and predicts bounding boxes and class probabilities directly from full images in one evaluation. The authors propose a unified model that integrates all stages of object detection into a single, jointly optimized model, allowing for fast and accurate object detection. The paper highlights the advantages of YOLO over traditional object detection methods, including its ability to make accurate predictions, learn generalizable representations of objects, and reason globally about the full image. The authors also compare YOLO with other state-of-the-art object detection systems, demonstrating its competitive performance on various benchmarks. Overall, the paper presents a significant contribution to the field of object detection, offering a fast, accurate, and efficient solution for real-world applications.\u001b[0m\n",
      "\u001b[36m+----------------------------------------------------------------------------------------------------+\u001b[0m\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import ReduceDocumentsChain, MapReduceDocumentsChain\n",
    "from langchain.chains import LLMChain, StuffDocumentsChain\n",
    "from time import perf_counter\n",
    "from langchain.schema import Document\n",
    "from termcolor import colored\n",
    "\n",
    "\n",
    "# Map\n",
    "map_template = \"\"\"The following is a set of parts within one long paper\n",
    "{docs}\n",
    "Based on the list of parts of the paper, please identify the main point in less than 3 sentences.\n",
    "Start the summarization with \"The main point of this section is\"\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "print(\"Init map chain...\")\n",
    "map_chain = LLMChain(llm=custom_llm, prompt=map_prompt)\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{doc_summaries}\n",
    "Take these and distill it into a final, consolidated summary of the main themes. \n",
    "Start the finalized summarization with \"The final summarization of this paper is\"\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "print(\"Init reduce chain...\")\n",
    "reduce_chain = LLMChain(llm=custom_llm, prompt=reduce_prompt)\n",
    "\n",
    "# Stuff documents using reduce chain\n",
    "print(\"Stuff documents using reduce chain...\")\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain, document_variable_name=\"doc_summaries\"\n",
    ")\n",
    "\n",
    "# Reduce documents chain\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    token_max=4000\n",
    ")\n",
    "\n",
    "# MapReduce chain setup\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    llm_chain=map_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    document_variable_name=\"docs\",\n",
    "    return_intermediate_steps=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Run the map phase separately\n",
    "print(\"Running map phase...\")\n",
    "map_results = []\n",
    "i=1\n",
    "for chunk in semantic_chunks:\n",
    "    # Assuming each chunk is a Document object with the paper content\n",
    "    result = map_chain({\"docs\": chunk.page_content})\n",
    "    map_results.append(result)\n",
    "    print(colored(f\"\\nIntermediate map result {i}:\", \"green\"))\n",
    "    print(result['text'])\n",
    "    print(\"\\n\")\n",
    "    i+=1\n",
    "print(\"Running reduce phase...\")\n",
    "\n",
    "\n",
    "try:\n",
    "    t1_start = perf_counter()\n",
    "    results = map_reduce_chain(semantic_chunks)\n",
    "    steps = results[\"intermediate_steps\"]\n",
    "    output = results[\"output_text\"]\n",
    "    t1_stop = perf_counter()\n",
    "    print(\"Elapsed time for reduce phase:\", round((t1_stop - t1_start), 2), \"seconds.\\n\") \n",
    "\n",
    "    # Create a box around the final output\n",
    "    box_width = 100\n",
    "    print(colored(\"+\" + \"-\" * box_width + \"+\", \"cyan\"))\n",
    "    print(colored(f\"{output}\", \"cyan\"))\n",
    "    print(colored(\"+\" + \"-\" * box_width + \"+\", \"cyan\"))\n",
    "    print(\"\\nDone.\")\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
